# Step 1 - Create an RDD based on all the weblogs
logs=sc.textFile("file:/home/training/training_materials/sparkdev/data/weblogs/*")
# map each request (line) to a pair (userid, 1), then sum the values
userreqs = logs \
   .map(lambda line: line.split()) \
   .map(lambda words: (words[2],1))  \
   .reduceByKey(lambda count1,count2: count1 + count2)
   
# Step 2 - Show the records for the 10 users with the highest counts
userreqs.map(lambda pair: (pair[1],pair[0])).sortByKey(False).take(10)

# Step 3 - Group IPs by user ID
userips = logs \
   .map(lambda line: line.split()) \
   .map(lambda words: (words[2],words[0])) \
   .groupByKey()
# print out the first 10 user ids, and their IP list
for (userid,ips) in userips.take(10):
   print userid, ":"
   for ip in ips: print "\t",ip

# Step 4a - Map account data to (userid,[values....])
accounts = sc.textFile("file:/home/training/training_materials/sparkdev/data/accounts.csv") \
   .map(lambda s: s.split(',')) \
   .map(lambda account: (account[0],account[1:]))

# Step 4b - Join account data with userreqs then merge hit count into valuelist   
accounthits = accounts.join(userreqs)

# Step 4c - Display userid, hit count, first name, last name for the first 5 elements
for (userid,(values,count)) in accounthits.take(5) : 
    print  userid, count, values[2],values[3]
   
# Challenge 1 - key accounts by postal/zip code
accountsByPCode = sc.textFile("file:/home/training/training_materials/sparkdev/data/accounts.csv") \
   .map(lambda s: s.split(','))\
   .keyBy(lambda account: account[8])
 
 # Challenge 2 - map account data to lastname,firstname  
namesByPCode = accountsByPCode\
   .mapValues(lambda account: account[4] + ',' + account[3]) \
   .groupByKey()

# Challenge 3 - print the first 5 zip codes and list the names 
for (pcode,names) in namesByPCode.sortByKey().take(5):
   print "---" ,pcode
   for name in names: print name


# Example code from the Working with RDDs chapter

# generate an array of random numbers and parallelize it to an RDD
from random import random
randomnumlist = [random.uniform(0,10) for _ in xrange(10000)] 
randomrdd = sc.parallelize(randomnumlist)
# calculate the average
print "Mean: %f" % randomrdd.mean()

#use flatMap to map a text file into individual words, and distinct to remove duplicates
myfile = "file:/home/training/training_materials/sparkdev/data/purplecow.txt"
sc.textFile(myfile) \
  .flatMap(lambda line: line.split()) \
  .distinct() \
  .collect()
  
# read a file with format userid[tab]firstname lastname
# users = sc.textFile(file) \
#    .map(lambda line: line.split('\t')) \
#    .map(lambda fields: (fields[0],fields[1]))

# parse a log file with userid in the third field, key by user ID
sc.textFile(logfile).keyBy(lambda line: line.split(' ')[2])

# Read zip code, latitude, longitude from a file, map to (zip,(lat,lon))
zipcoordfile = "file:/home/training/training_materials/sparkdev/data/latlon.tsv"
zipcoords = sc.textFile(zipcoordfile) \
    .map(lambda s: \
      (s.split()[0],(s.split()[1],s.split()[2])))
for (zip,coords) in zipcoords.take(5): print "Zip code: ", zip, " at ", coords

# order file format: orderid:skuid,skuid,skuid...
# map to RDD of skuids keyed by orderid
orderfile = "file:/home/training/training_materials/sparkdev/data/orderskus.txt"
orders = sc.textFile(orderfile) \
  .map(lambda s: s.split('\t')) \
  .map(lambda fields: (fields[0],fields[1])) \
  .flatMapValues(lambda skus:skus.split(':'))

for pair in orders.take(5): print pair

# count words in the file
wordfile = "file:/home/training/training_materials/sparkdev/data/purplecow.txt"
counts = sc.textFile(wordfile) \
    .flatMap(lambda s: s.split()) \
    .map(lambda w: (w,1)) \
    .reduceByKey(lambda v1,v2: v1+v2)
for pair in counts.take(5): print pair

